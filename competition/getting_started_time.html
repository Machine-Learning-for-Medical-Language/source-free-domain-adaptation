<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"></head><body><h1 style="color: grey;">Getting Started: Time</h1>
<style>
.results {border-collapse: collapse;}
.results th, td {padding: 5px;}
.results th {background-color: #f2f2f2;}
.results tr:nth-child(even) {background-color: #f2f2f2;} 
.results tr:nth-child(odd) {background-color: #ffffff;} 
.codestyle {background-color: #f2f2f2; padding: 3px; color: #666666;}
.prestyle {background-color: #f2f2f2; padding: 5px;  padding-left: 30px;}
</style>

<h2>Get and prepare practice data</h2>

<p>The trial data for the practice phase consists of 99 articles from the <em>AQUAINT</em>, <em>TimeBank</em> and <em>te3-platinum</em> subsets of <em>TempEval-2013</em>, i.e. <em>"Newswire"</em> domain.</p>
<p>You can automatically download and prepare the input data for this phase running the <code class="codestyle">prepare_time_dataset.py</code> script available in the <a href="https://github.com/Machine-Learning-for-Medical-Language/source-free-domain-adaptation" target="_blank">task repository</a>:</p>

<pre class="prestyle">

$ git clone https://github.com/Machine-Learning-for-Medical-Language/source-free-domain-adaptation.git && cd source-free-domain-adaptation

$ python3 prepare_time_dataset.py /path/to/practice_data/

</pre>

<p>This will create a <code class="codestyle">time</code> directory in <code class="codestyle">/path/to/practice_data/</code> containing the plain text of the <a href="https://github.com/Machine-Learning-for-Medical-Language/source-free-domain-adaptation/blob/master/practice_time_documents.txt" target="_blank">documents used in this task</a>.</p>

<h2>Get model and predictions on the development data</h2>

<p>The baseline for the time expression recognition is based on the pytorch implementation of <a href="https://huggingface.co/transformers/model_doc/roberta.html" target="_blank">RoBERTa</a> by <em>Hugging Face</em>. We have used the <code class="codestyle"><a href="https://huggingface.co/transformers/model_doc/roberta.html?#robertafortokenclassification" target="_blank">RobertaForTokenClassification</a></code> architecture from <em>Hugging Face/transformers</em> library to fine-tune <code class="codestyle"><a href="https://huggingface.co/roberta-base" target="_blank">roberta-base</a></code> on 25,000+ time expressions in de-identified clinical notes. The resulting model is a sequence tagger that we have made available in <em>Hugging Face</em> model hub: <a href="https://huggingface.co/clulab/roberta-timex-semeval" target="_blank">clulab/roberta-timex-semeval</a>.  The following table shows the <em>in-domain</em> and <em>out-of-domain (practice_data)</em> performances:
<p style="padding-left: 30px;">
<table class="results">
<thead>
<tr>
<th></th>
<th>P</th>
<th>R</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>in-domain_data</td>
<td>0.967</td>
<td>0.968</td>
<td>0.968</td>
</tr>
<tr>
<td>practice_data</td>
<td>0.775</td>
<td>0.768</td>
<td>0.771</td>
</tr>
</tbody>
</table>
</p>


<p>The task repository contains scripts to load and run the model: <code class="codestyle"><a href="https://github.com/Machine-Learning-for-Medical-Language/source-free-domain-adaptation/tree/master/baselines/time-baseline" target="_blank">time-baseline</a></code>. These scripts are based on the <em>Hugging Facet/transformers</em> library that allows easily incorporating the model into the code. For example, the following snippet shows how to load the model and its tokenizer:

<pre class="prestyle">

from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification

config = AutoConfig.from_pretrained("clulab/roberta-timex-semeval")

tokenizer = AutoTokenizer.from_pretrained("clulab/roberta-timex-semeval", config=self.config, use_fast=True)

model = AutoModelForTokenClassification.from_pretrained("clulab/roberta-timex-semeval", config=self.config)

</pre>

<p>The first time you run such code, the model will be automatically downloaded in your computer. The scripts also include the basic functionality to read the input data and produce the output <em>Anafora</em> annotations. You can use the <code class="codestyle">run_time.py</code> script to parse raw text and obtain time expressions. For example, to process the <em>AQUAINT</em> subset from <em>TempEval-2013</em>, just run:</p>

<pre class="prestyle">

  $ cd baselines/time-baseline
  
  $ python3 run_time.py -p /path/to/practice_data/time/AQUAINT -o /path/to/output/AQUAINT

</pre>

<p>This will create one directory per document in <code class="codestyle">output/AQUAINT</code> containing one <code class="codestyle">.xml</code> file with predictions in <em>Anafora</em> format. You can also run the <code class="codestyle">runt_train.py</code> script to continue training <code class="codestyle">clulab/roberta-timex-semeval</code>:</p>

<pre class="prestyle">

  $ python3 run_train.py -t /path/to/train-data -s /path/to/save-model

</pre>

<p>The <code class="codestyle">train-data</code> directory must follow a similar structure to the <code class="codestyle">practice_data/time</code> folder and include, for each document, a the raw text file (with no extension) and an <em>Anafora</em> annotation file (with <code class="codestyle">.xml</code> extension). After running the training, the <code class="codestyle">save-model</code> directory will contain two sub-folders, <code class="codestyle">logs</code>, with a set of log files that can be visualized with TensorBoard, and <code class="codestyle">results</code>, that contains all the checkpoints saved during the training and three files (<code class="codestyle">pytorch_model.bin</code>, <code class="codestyle">training_args.bin</code> and <code class="codestyle">config.json</code>) with the configuration and weights of the final model.</p>


<h2>Upload predictions to CodaLab</h2><p style="padding-left: 30px;"></p>


<p>To make a submission for the practice phase, you must create a directory with the output for the 99 documents following the structure explained in the <strong>Output Format</strong> section.</p>

</body></html>

