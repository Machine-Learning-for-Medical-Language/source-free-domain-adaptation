<h1 style="color: grey;">Data, Models and Evaluation</h1>
<h2>Data<strong><br /></strong></h2>
<p>Since the scenearion proposed by SemEval-2021 Task 10 is domain adaptation with no access to the source data, no annotated training set is distributed. Instead, participants are provided with models trained on that source data, the development data representing a new domain on which participants can explore domain adaptation algorithms, and the test data representing another new domain on which the participant's approaches will be evaluated.</p>
<p>The <strong>negation detection</strong> track uses as development data the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare, containing 7,038 entities labeled with an assertion status in the set Asserted, Negated, Uncertain,&nbsp; Hypothetical, Conditional,&nbsp; FamilyRelated. While the i2b2 2010 Challenge data requires a Data Use Agreement with Partners HealthCare, these are typically straightforward to execute (this data was already part of a shared task), so we do not expect significant issues for either participants or future researchers. For&nbsp;<strong>time expression recognition</strong>, the development data is the annotated news portion of the SemEval 2018 Task 6 data. The source text is from the freely available <a href="https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html" target="_blank">TimeBank</a>, and the 2,000+ time entity annotations are store in in&nbsp;<a href="https://github.com/weitechen/anafora">Anafora</a> XML format.</p>
<p><span style="text-decoration: line-through;">The test data will be a portion of the <a href="https://mimic.physionet.org/" target="_blank">MIMIC III corpus</a>, which contains manually de-identified notes for patients from the intensive care unit of Beth Israel Deaconess Medical Center. Access to the MIMIC data requires participants to complete a CITI "Data or Specimens Only Research" online course, and then make an online request through PhysioNet. The course takes only a couple of hours online, and access requests are typically approved within a few days.</span></p>
<p>&nbsp;</p>
<h2>Models<strong><br /></strong></h2>
<p>Paricipants are provided with trained models for Tracks 1 and 2.&nbsp; In both cases, we have used the <em>RoBERTa-base</em> (Liu et al., 2019) pretrained model included in the <a href="https://github.com/huggingface/transformers" target="_blank">Huggingface Transformers</a> library:</p>
<ul>
<li><strong>Track 1</strong>: For negation detection, we provide "span-in-context" classification model, fine-tuned on the 10,259 instances (902 negated) in the SHARP Seed dataset of de-identified clinical notes from Mayo Clinic, which the organizers have access to but cannot currently be distributed. In the SHARP data, clinical events are marked with a boolean polarity indicator, with values of either <em>asserted </em>or <em>negated</em>.</li>
<li><strong>Track 2</strong>: For time expression recognition,&nbsp; we provide a sequence tagging model, fine-tuned on the 25,000+ time expressions in the clinical portions of the SemEval 2018 Task 6, that are available to us, but are currently very difficult to gain access to,&nbsp; due to the complex data use agreements necessary</li>
</ul>
<p>&nbsp;</p>
<h2>Evaluation metrics</h2>
<p>The scores for each track are calculated as follows:&nbsp;</p>
<ul>
<li><strong>Track 1:&nbsp;</strong>Negation detection will be evaluated using the standard precision, recall and F<sub>1</sub> scores as used in most published work.</li>
<li><strong><strong>Track </strong>2:</strong> Time expression recognition will be evaluated using the standard precision, recall and F<sub>1</sub> previously used for the entity-finding portion of SemEval 2018 Task 6.</li>
</ul>
<h2>&nbsp;</h2>
<h2>System Output Format</h2>
<p>For <strong>negation detection </strong>track, ... For <strong>time expression recognition</strong>, your system must produce Anafora XML format files. Your directory structure should follow the following organization:</p>
<table style="border-color: #e6ecff; background-color: #eaeaea; border-width: 0px; height: 200px; width: 500px;" border="0" align="center">
<tbody>
<tr>
<td>
<ul>
<li>negation</li>
<ul>
<li>system.tsv</li>
</ul>
<li>time</li>
<ul>
<li>Set</li>
<ul>
<li>doc_001</li>
<ul>
<li>doc_001.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_002</li>
<ul>
<li>doc_002.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_003</li>
<ul>
<li>doc_003.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_004</li>
<ul>
<li>doc_004.TimeNorm.system.completed.xml</li>
</ul>
<li>...</li>
</ul>
</ul>
</ul>
</td>
</tr>
</tbody>
</table>
<p style="text-align: left;">&nbsp;</p>
<p style="text-align: left;">Make sure that you comply with following rules when you create &nbsp;your output directory:</p>
<ul>
<li>The root must contain only the track directories, namely, <em>"negation"</em> and <em>"time"</em>.</li>
<li>If you don't produce output for any of the tracks, do not include its corresponding directory.</li>
<li>In the<em> "negation"</em> directory, include a single tsv file with the name "system.tsv".</li>
<li>The<em> "time"</em> directory must follow the same structure and names as in the dataset:</li>
<ul>
<li>Each domain directory must contain only the corresponding document directories and their names must be same as in the dataset.</li>
<li>If you don't produce output for any of the documents, do not include its corresponding directory.</li>
<li>Each document directory must contain only the corresponding annotation file.&nbsp;</li>
<li>The name of the annotation files must match the document name and they must include a "xml" extension.</li>
</ul>
</ul>
<h2>References</h2>
<p>Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., and Stoyanov V. <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank">Roberta: A robustly optimized bert pretraining approach</a>. arXiv preprint. 2019.</p>
<p>&nbsp;</p>
