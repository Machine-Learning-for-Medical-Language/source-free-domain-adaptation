<h1 style="color: grey;">Data, Models and Evaluation</h1>
<h2>Data<strong><br /></strong></h2>
<p>Since the scenario proposed by SemEval-2021 Task 10 is domain adaptation with no access to the source data, no annotated training set is distributed. Instead, participants are provided with models trained on that source data, the development data representing a new domain on which participants can explore domain adaptation algorithms, and the test data representing another new domain on which the participant's approaches will be evaluated.</p>
<p>The <strong>negation detection</strong> track uses as development data the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare, containing 2886 unlabeled train instances (entities in sentence context), and 5545 dev instances with a corresponding labeling for with negation status. The original i2b2 data set had multi-label annotations in the set Asserted, Negated, Uncertain,&nbsp; Hypothetical, Conditional,&nbsp; FamilyRelated - to align with other challenge datasets we have kept the Negated category but mapped all other categories to "Not negated." The i2b2 2010 Challenge data requires a Data Use Agreement with Partners HealthCare, in order to access the development data, participants must first obtain access through the <a href="https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/">n2c2/DBMI Data Portal</a>. After downloading the 2010 data, participants can then run scripts that are in the Github repo for this task. For&nbsp;<strong>time expression recognition</strong>, the development data is the annotated news portion of the SemEval 2018 Task 6 data. The source text is from the freely available <a href="https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html" target="_blank">TimeBank</a>, and the 2,000+ time entity annotations are store in in&nbsp;<a href="https://github.com/weitechen/anafora">Anafora</a> XML format.</p>
<p>Participants should also <a href="https://mimic.physionet.org/" target="_blank">obtain access to the MIMIC III corpus</a>, as it may be used for one or both f the test sets. Access to the MIMIC data requires participants to complete a CITI "Data or Specimens Only Research" online course, and then make an online request through PhysioNet. The course takes only a couple of hours online, and access requests are typically approved within a few days.</p>

<h2>Models<strong><br /></strong></h2>
<p>Paricipants are provided with trained models for Tracks 1 and 2.&nbsp; In both cases, we have used the <em>RoBERTa-base</em> (Liu et al., 2019) pretrained model included in the <a href="https://github.com/huggingface/transformers" target="_blank">Huggingface Transformers</a> library:</p>
<ul>
<li><strong>Track 1</strong>: For negation detection, we provide "span-in-context" classification model, fine-tuned on the 10,259 instances (902 negated) in the SHARP Seed dataset of de-identified clinical notes from Mayo Clinic, which the organizers have access to but cannot currently be distributed (models are approved to be distributed). In the SHARP data, clinical events are marked with a boolean polarity indicator, with values of either <em>asserted </em>or <em>negated</em>.</li>
<li><strong>Track 2</strong>: For time expression recognition,&nbsp; we provide a sequence tagging model, fine-tuned on the 25,000+ time expressions in the clinical portions of the SemEval 2018 Task 6, that are available to us, but are currently very difficult to gain access to,&nbsp; due to the complex data use agreements necessary</li>
</ul>
<p>&nbsp;</p>
<h2>Evaluation metrics</h2>
<p>The scores for each track are calculated as follows:&nbsp;</p>
<ul>
<li><strong>Track 1:&nbsp;</strong>Negation detection will be evaluated using the standard precision, recall and F<sub>1</sub> scores as used in most published work -- recall points are gained by correctly predicting that a negated entity is negated, precision points are obtained if a predicted negation is correct.</li>
<li><strong><strong>Track </strong>2:</strong> Time expression recognition will be evaluated using the standard precision, recall and F<sub>1</sub> previously used for the entity-finding portion of SemEval 2018 Task 6.</li>
</ul>
<h2>&nbsp;</h2>
<h2>System Output Format</h2>
<p>For the <strong>negation detection </strong>track, the output format is simply one classifier output per line, where the lines correspond to the lines in the input. A prediction of "Negated" should be output as 1, while a prediction of "Not negated" should be output as -1. For <strong>time expression recognition</strong>, your system must produce Anafora XML format files. Your directory structure should follow the following organization:</p>
<table style="border-color: #e6ecff; background-color: #eaeaea; border-width: 0px; height: 200px; width: 500px;" border="0" align="center">
<tbody>
<tr>
<td>
<ul>
<li>negation</li>
<ul>
<li>system.tsv</li>
</ul>
<li>time</li>
<ul>
<li>Set</li>
<ul>
<li>doc_001</li>
<ul>
<li>doc_001.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_002</li>
<ul>
<li>doc_002.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_003</li>
<ul>
<li>doc_003.TimeNorm.system.completed.xml</li>
</ul>
</ul>
</ul>
<ul>
<ul>
<li>doc_004</li>
<ul>
<li>doc_004.TimeNorm.system.completed.xml</li>
</ul>
<li>...</li>
</ul>
</ul>
</ul>
</td>
</tr>
</tbody>
</table>
<p style="text-align: left;">&nbsp;</p>
<p style="text-align: left;">Make sure that you comply with following rules when you create &nbsp;your output directory:</p>
<ul>
<li>The root must contain only the track directories, namely, <em>"negation"</em> and <em>"time"</em>.</li>
<li>If you don't produce output for any of the tracks, do not include its corresponding directory.</li>
<li>In the<em> "negation"</em> directory, include a single tsv file with the name "system.tsv".</li>
<li>The<em> "time"</em> directory must follow the same structure and names as in the dataset:</li>
<ul>
<li>Each domain directory must contain only the corresponding document directories and their names must be same as in the dataset.</li>
<li>If you don't produce output for any of the documents, do not include its corresponding directory.</li>
<li>Each document directory must contain only the corresponding annotation file.&nbsp;</li>
<li>The name of the annotation files must match the document name and they must include a "xml" extension.</li>
</ul>
</ul>
<h2>References</h2>
<p>Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., and Stoyanov V. <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank">Roberta: A robustly optimized bert pretraining approach</a>. arXiv preprint. 2019.</p>
<p>&nbsp;</p>
